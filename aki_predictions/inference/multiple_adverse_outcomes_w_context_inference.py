"""
Runs inference on a saved model, generated by multiple_adverse_outcomes_training.
The model checkpoint artefacts must be defined in a checkpoint directory for weights to be reloaded correctly.
"""
import sys
import logging
import time
from pathlib import Path

import numpy as np


from aki_predictions.ehr_prediction_modeling import config as experiment_config
from aki_predictions.ehr_prediction_modeling import types
from aki_predictions.training.multiple_adverse_outcomes_training import run
from aki_predictions.data_processing import CAP_CENTILES


def _get_config(data_dir, checkpoint_dir="", root_logger=None, steps=None, **kwargs):
    if root_logger is None:
        root_logger = logging.getLogger()
    root_logger.info(data_dir)

    if steps is None:
        steps = 220000

    if CAP_CENTILES:
        capped_string = ""
    else:
        capped_string = "_uncapped"

    data_locs_dict = {
        "records_dirpath": data_dir,
        "train_filename": f"ingest_records_output_lines_inference{capped_string}.jsonl",
        "valid_filename": f"ingest_records_output_lines_inference{capped_string}.jsonl",
        "test_filename": f"ingest_records_output_lines_inference{capped_string}.jsonl",
        "calib_filename": f"ingest_records_output_lines_inference{capped_string}.jsonl",
        "category_mapping": "category_mapping.json",
        "feature_mapping": "feature_mapping.json",
        "numerical_feature_mapping": "numerical_feature_mapping.json",
        "metadata_mapping": "metadata_mapping.json",
        "missing_metadata_mapping": "missing_metadata_mapping.json",
    }
    context_nact_dict = {"diagnosis": 2, "ethnic_origin": 1, "method_of_admission": 1, "sex": 1, "year_of_birth": 1}
    nact_dict = {
        types.FeatureTypes.PRESENCE_SEQ: 10,
        types.FeatureTypes.NUMERIC_SEQ: 3,
        types.FeatureTypes.CATEGORY_COUNTS_SEQ: 3,
        **context_nact_dict,
    }
    context_features = list(context_nact_dict.keys())
    var_len_context_features = ["diagnosis"]
    fixed_len_context_features = [
        cont_feat for cont_feat in context_features if cont_feat not in var_len_context_features
    ]
    identity_lookup_features = [
        types.FeatureTypes.CATEGORY_COUNTS_SEQ,
        types.FeatureTypes.ETHNIC_ORIGIN,
        types.FeatureTypes.METHOD_OF_ADMISSION,
        types.FeatureTypes.SEX,
    ]
    shared_config_kwargs = {
        "tasks": (types.TaskNames.ITU_OUTCOME, types.TaskNames.DIALYSIS_OUTCOME, types.TaskNames.MORTALITY_OUTCOME),
        "context_features": context_features,
        "fixed_len_context_features": fixed_len_context_features,
        "var_len_context_features": var_len_context_features,
        "identity_lookup_features": identity_lookup_features,
    }
    config = experiment_config.get_config(
        nact_dict=nact_dict,
        data_locs_dict=data_locs_dict,
        num_steps=steps,  # 101 for testing
        eval_num_batches=None,  # to allow exiting via out of range error
        checkpoint_every_steps=100,  # 1000 for full dataset, 100 for testing
        summary_every_steps=1000,  # 1000 for full dataset, 100 for testing
        eval_every_steps=100,  # 1000 for full dataset, 100 for testing
        shared_config_kwargs=shared_config_kwargs,
        shuffle=True,
        run_occlusion_analysis=False,
        checkpoint_dir=checkpoint_dir,
        threshold_range=np.concatenate((np.arange(0.001, 0.01, 0.001), np.arange(0.01, 1, 0.01)), axis=0),
        run_inference=True,
        **kwargs,
    )
    return config


def main(model_dir, data_dir, steps):
    """Run experiment."""
    root_logger = logging.getLogger(__name__)

    config = _get_config(data_dir=data_dir, checkpoint_dir=model_dir, root_logger=root_logger, steps=steps)
    run(config, root_logger=root_logger)


if __name__ == "__main__":
    output_dir = sys.argv[1]
    model_dir = sys.argv[2]
    data_dir = sys.argv[3]
    steps = sys.argv[4]

    if data_dir is None:
        data_dir = str(Path(__file__).resolve().parents[2] / "data" / "data_ingest_index_full_2022-07-11-100305")

    timestamp = time.strftime("%Y-%m-%d-%H%M%S")
    artifacts_dir = Path(output_dir)
    if artifacts_dir.is_dir() is False:
        artifacts_dir.mkdir(parents=True, exist_ok=True)

    # Configure logging
    log_formatter = logging.Formatter("%(asctime)s [%(name)s] [%(levelname)-5.5s]  %(message)s")
    root_logger = logging.getLogger()

    file_handler = logging.FileHandler("{0}/{1}.log".format(artifacts_dir, f"{timestamp}_inference_log.txt"))
    file_handler.setFormatter(log_formatter)
    root_logger.addHandler(file_handler)

    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(log_formatter)
    root_logger.addHandler(console_handler)
    root_logger.setLevel(logging.DEBUG)

    main(model_dir, data_dir, steps)
