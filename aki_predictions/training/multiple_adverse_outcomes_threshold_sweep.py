"""
Runs threshold sweep on a saved model, generated by multiple_adverse_outcomes_training.
The model checkpoint artefacts must be defined in a checkpoint directory for weights to be reloaded correctly.

Note:
    Currently utilises the default checkpoints directory.
    After running the checkpoints file must be updated to reflect the original final checkpoint
     (matching the step count)
    The threshold step size is defined in the master training script.
"""
import sys
import logging
import time
from pathlib import Path

import numpy as np

from aki_predictions.ehr_prediction_modeling import config as experiment_config
from aki_predictions.ehr_prediction_modeling import types
from aki_predictions.training.multiple_adverse_outcomes_training import run
from aki_predictions.data_processing import CAP_CENTILES


def _get_config(data_dir, checkpoint_dir="", root_logger=None, steps=None, **kwargs):
    if root_logger is None:
        root_logger = logging.getLogger()
    root_logger.info(data_dir)

    if steps is None:
        # Default training run length (to match training run and allow selection of the correct model)
        steps = 220000

    if CAP_CENTILES:
        capped_string = ""
    else:
        capped_string = "_uncapped"

    data_locs_dict = {
        "records_dirpath": data_dir,
        "train_filename": f"ingest_records_output_lines_train{capped_string}.jsonl",
        "valid_filename": f"ingest_records_output_lines_validate{capped_string}.jsonl",
        "test_filename": f"ingest_records_output_lines_test{capped_string}.jsonl",
        "calib_filename": f"ingest_records_output_lines_calib{capped_string}.jsonl",
        "category_mapping": "category_mapping.json",
        "feature_mapping": "feature_mapping.json",
        "numerical_feature_mapping": "numerical_feature_mapping.json",
        "sequence_giveaways": "sequence_giveaways.json",
    }
    shared_config_kwargs = {
        "tasks": (types.TaskNames.ITU_OUTCOME, types.TaskNames.DIALYSIS_OUTCOME, types.TaskNames.MORTALITY_OUTCOME)
    }
    config = experiment_config.get_config(
        data_locs_dict=data_locs_dict,
        num_steps=steps,  # 2 for testing
        eval_num_batches=None,  # to allow exiting via out of range error
        checkpoint_every_steps=1000,  # 1000 for full dataset, 1 for testing
        summary_every_steps=1000,  # 1000 for full dataset, 1 for testing
        eval_every_steps=2000,  # 1000 for full dataset, 1 for testing
        shared_config_kwargs=shared_config_kwargs,
        shuffle=False,
        curriculum_starting_epoch=2,
        run_occlusion_analysis=False,
        run_threshold_sweep=True,
        threshold_range=np.concatenate((np.arange(0.001, 0.01, 0.001), np.arange(0.01, 1, 0.01)), axis=0),
        checkpoint_dir=checkpoint_dir,
        expect_giveaways=True,
        **kwargs,
    )
    return config


def main(output_dir, data_dir, steps):
    """Run threshold sweep."""
    root_logger = logging.getLogger(__name__)

    config = _get_config(data_dir=data_dir, checkpoint_dir=output_dir, root_logger=root_logger, steps=steps)
    assert config.run_threshold_sweep is True
    run(config, root_logger=root_logger)


if __name__ == "__main__":
    output_dir = sys.argv[1]
    data_dir = sys.argv[2]
    steps = int(sys.argv[3])

    if data_dir is None:
        data_dir = str(Path(__file__).resolve().parents[2] / "data" / "data_ingest_index_full_2022-07-11-100305")

    timestamp = time.strftime("%Y-%m-%d-%H%M%S")
    artifacts_dir = Path(output_dir)
    if artifacts_dir.is_dir() is False:
        artifacts_dir.mkdir(parents=True, exist_ok=True)
    # Configure logging
    log_formatter = logging.Formatter("%(asctime)s [%(name)s] [%(levelname)-5.5s]  %(message)s")
    root_logger = logging.getLogger()

    file_handler = logging.FileHandler("{0}/{1}.log".format(artifacts_dir, f"{timestamp}_threshold_sweep_log.txt"))
    file_handler.setFormatter(log_formatter)
    root_logger.addHandler(file_handler)

    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(log_formatter)
    root_logger.addHandler(console_handler)
    root_logger.setLevel(logging.DEBUG)

    main(artifacts_dir, data_dir, steps)
