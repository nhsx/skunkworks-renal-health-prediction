{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Modified by Roke Manor Research to support a runnable notebook - with additions to show use of protocol buffer compilation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import version\n",
    "\n",
    "version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing proto buffers\n",
    "```sudo apt-get install protobuf-compiler```\n",
    "\n",
    "```protoc -I=aki_predictions/ehr_prediction_modeling --python_out=aki_predictions/ehr_prediction_modeling aki_predictions/ehr_prediction_modeling/proto/*.proto```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we load and display data at various processing stages: raw `FakeRecord`, sequential `FakePatient`, vectorized `GenericEventSequence`, and sparse encoded `tf.SequenceExample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "# @title Imports\n",
    "# Protocol buffers must first be compiled using `protoc`: see https://developers.google.com/protocol-buffers/docs/pythontutorial for details.from ehr_prediction_modeling.proto import fake_records_pb2\n",
    "from aki_predictions.ehr_prediction_modeling.proto import (  # Missing from original notebook.\n",
    "    fake_generic_representation_pb2,\n",
    "    fake_patient_pb2,\n",
    "    fake_records_pb2,\n",
    ")\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "data_dirpath = os.getcwd()  # \"path/to/directory/that/stores/data\"  # @param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = os.path.join(data_dirpath, \"fake_raw_records.pb\")\n",
    "\n",
    "with open(raw_data_path, \"rb\") as f:\n",
    "    records = fake_records_pb2.FakeRecords.FromString(f.read()).records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"There are {len(records)} records in the fake dataset.\\nOne example is:\\n\\n{records[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_path = os.path.join(data_dirpath, \"fake_patients.pb\")\n",
    "\n",
    "with open(patient_path, \"rb\") as f:\n",
    "    patients = fake_patient_pb2.FakePatients.FromString(f.read()).patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"There are {len(patients)} patients in the fake dataset.\\nOne example is:\\n\\n{patients[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_admissions = []\n",
    "for patient in patients:\n",
    "    for episode in patient.episodes:\n",
    "        if episode.WhichOneof(\"episode_type\") == \"admission\":\n",
    "            all_admissions.append(episode.admission)\n",
    "\n",
    "print(\n",
    "    f\"There are {len(all_admissions)} admissions in the fake dataset.\\nOne example is:\\n\\n{all_admissions[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clinical_events = []\n",
    "for patient in patients:\n",
    "    for episode in patient.episodes:\n",
    "        if episode.WhichOneof(\"episode_type\") == \"admission\":\n",
    "            for event in episode.admission.clinical_events:\n",
    "                all_clinical_events.append(event)\n",
    "\n",
    "print(\n",
    "    f\"There are {len(all_clinical_events)} clinical events in admission in the fake dataset.\\nOne example is:\\n\\n{all_clinical_events[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read GenericEventSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_path = os.path.join(data_dirpath, \"vectorized\", \"fake_vectorized_samples.pb\")\n",
    "\n",
    "with open(vectorized_path, \"rb\") as f:\n",
    "    event_sequences = (\n",
    "        fake_generic_representation_pb2.FakeGenericEventSequences.FromString(\n",
    "            f.read()\n",
    "        ).generic_event_sequences\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"There are {len(event_sequences)} event sequences in the fake dataset.\\nOne example is:\\n\\n{event_sequences[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.SequenceExample`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"train\"  # @param\n",
    "seqex_path = os.path.join(data_dirpath, f\"standardize/{split}.tfrecords\")\n",
    "filenames = [seqex_path]\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "iterator = raw_dataset.make_initializable_iterator()\n",
    "init = tf.initialize_all_variables()\n",
    "batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seqexs = []\n",
    "with tf.train.MonitoredTrainingSession() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    sess.run(init)\n",
    "    while True:\n",
    "        all_seqexs.append(tf.train.SequenceExample.FromString(sess.run(batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"There are {len(all_seqexs)} sequence examples in the fake dataset for {split} split.\\nOne example is:\\n\\n{all_seqexs[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bccd73cbd088116dc4dc1711f8b4948de4c9c85be53b8ff6e395c61a9241f153"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
